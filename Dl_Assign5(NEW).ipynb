{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_vA6HGADMDs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Lambda\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n",
        "data=\"\"\" We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\"\n",
        "\n",
        "\n",
        "sentences=data.split(\".\")\n",
        "sentences\n",
        "\n",
        "\n",
        "clean_sentence=[]\n",
        "\n",
        "for sentence in sentences:\n",
        "    if sentence==\"\" :\n",
        "            continue\n",
        "\n",
        "    sentence=re.sub(\"[^A-Z0-9a-z]+\", \" \",sentence)\n",
        "\n",
        "    sentence=re.sub(r\"(?:^| )\\w(?:$| )\",\" \",sentence).strip()\n",
        "\n",
        "    sentence=sentence.lower()\n",
        "    clean_sentence.append(sentence)\n",
        "\n",
        "clean_sentence\n",
        "\n",
        "\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_sentence)\n",
        "sequences=tokenizer.texts_to_sequences(clean_sentence)\n",
        "sequences\n",
        "\n",
        "index_to_word_map={}\n",
        "word_to_index_map={}\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    words_in_sentence=clean_sentence[i].split()\n",
        "\n",
        "    for j, value in enumerate(sequence):      #j represents the index within the sentence, and value is the word's numerical index from sequence.\n",
        "        index_to_word_map[value]=words_in_sentence[j]\n",
        "        word_to_index_map[words_in_sentence[j]]=value\n",
        "\n",
        "index_to_word_map\n",
        "word_to_index_map\n",
        "\n",
        "\n",
        "vocab_size=len(tokenizer.word_index)+1\n",
        "emb_size=10\n",
        "context_size=2\n",
        "\n",
        "\n",
        "contexts=[]\n",
        "targets=[]\n",
        "for sequence in sequences:\n",
        "  for i in range(context_size, len(sequence)-context_size):\n",
        "    target=sequence[i]\n",
        "    context=sequence[i-2],sequence[i-1],sequence[i+1],sequence[i+2]\n",
        "    contexts.append(context)\n",
        "    targets.append(target)\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "  words=[]\n",
        "\n",
        "  target=index_to_word_map.get(targets[i])\n",
        "\n",
        "  for j in contexts[i]:\n",
        "    words.append(index_to_word_map.get(j))\n",
        "\n",
        "  print(words ,\"=>\", target)\n",
        "\n",
        "\n",
        "\n",
        "X=np.array(contexts)\n",
        "Y=np.array(targets)\n",
        "\n",
        "model=Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=emb_size, input_length=2*context_size),\n",
        "    Lambda(lambda x: tf.reduce_mean(x, axis=1) ),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(units=vocab_size,activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.fit(X,Y,epochs=200)\n",
        "\n",
        "test_sentenses = [\n",
        "    \"we are to study\",\n",
        "    \"create programs direct processes\",\n",
        "    \"spirits process study program\",\n",
        "    \"idea study people create\"\n",
        "]\n",
        "\n",
        "\n",
        "for test_sentense in test_sentenses:\n",
        "    test_words = test_sentense.split(\" \")\n",
        "\n",
        "    x_test = []\n",
        "    for i in test_words:\n",
        "        x_test.append(word_to_index_map.get(i))\n",
        "    x_test = np.array([x_test])\n",
        "\n",
        "    test_predictions = model.predict(x_test)\n",
        "    y_pred = np.argmax(test_predictions[0])\n",
        "    print(\"Predictons: \", test_words, \" => \", index_to_word_map.get(y_pred))\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n"
      ]
    }
  ]
}